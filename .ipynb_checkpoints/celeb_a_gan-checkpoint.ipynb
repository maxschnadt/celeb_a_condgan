{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f88b2c0c-f363-4300-8b96-0dd951bdf27d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import the packages and define the (environment) variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3324fe-d7d7-419d-a765-8e4fc28c66ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642d2489-6d70-4c7f-aee6-4ca336bd1dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"F:\\\\celeb_a\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf14baf-73e6-4a20-9b4b-dc4c23274a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = dir_path + \"img_align_celeba\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26c2ea5-7e54-4e2c-8529-f0caa5b7c0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = dir_path + \"celeb_a_attrs.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea901bd-2ae9-4802-806a-34b99f27dacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "noise_size = 54600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5641ea0d-44c1-4c79-a86f-7d0b84aa1d2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create the dataset pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6965e25b-11be-4b27-a1b1-fdd6eb0995d8",
   "metadata": {},
   "source": [
    "## Read the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd41ffe3-fc9e-48cb-9d60-9d9428e908d7",
   "metadata": {},
   "source": [
    "*Only needed for Conditional GANs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df1f7b7-55e7-4640-b5e0-40a42c509bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = pd.read_csv(labels_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba93b38-effb-447d-95a2-1fcaeb01f327",
   "metadata": {},
   "source": [
    "## Create a list of all the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ffce5-a13d-459d-afd7-c2d991fad937",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file_list = []\n",
    "\n",
    "for rootpath, _, filename in os.walk(images_path):\n",
    "    for name in filename:\n",
    "        fullpath = os.path.join(rootpath, name)\n",
    "        image_file_list.append(fullpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718ef165-b9cf-47af-9710-9cf368d1867b",
   "metadata": {},
   "source": [
    "## Make a tf.data dataset based on the file list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7075b2f8-1abe-43cb-be54-903fce832cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset = tf.data.Dataset.from_tensor_slices(image_file_list)\n",
    "image_dataset = image_dataset.map(lambda item : tf.io.read_file(filename=item))\n",
    "image_dataset = image_dataset.map(lambda item : tf.io.decode_jpeg(contents=item))\n",
    "image_dataset = image_dataset.map(lambda item : tf.cast(x=item, dtype=tf.float32))\n",
    "image_dataset = image_dataset.map(lambda item : tf.divide(x=item, y=255.0)) # Scale to [0.0, 1.0]\n",
    "image_dataset = image_dataset.shuffle(buffer_size=10000, reshuffle_each_iteration=False)\n",
    "image_dataset = image_dataset.batch(batch_size=batch_size, drop_remainder=True, num_parallel_calls=6)\n",
    "image_dataset = image_dataset.prefetch(buffer_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03746f18-92f5-45b7-b23a-b44a21acdae0",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b1f101-2f1b-4a61-8bdc-d08f834a6d05",
   "metadata": {},
   "source": [
    "## Build the generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa578ea-80df-426b-84c6-974835dfc851",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_in = keras.layers.Input(shape=[54600])\n",
    "g = keras.layers.Reshape(target_shape=(26, 21, 100))(gen_in)\n",
    "g = keras.layers.Conv2DTranspose(filters=30, kernel_size=(4), strides=(2), padding=\"valid\", kernel_initializer=\"lecun_normal\")(g)\n",
    "g = keras.layers.BatchNormalization()(g)\n",
    "g = keras.layers.Activation(activation=\"elu\")(g)\n",
    "g = keras.layers.Conv2DTranspose(filters=60, kernel_size=(3), strides=(2), padding=\"valid\", kernel_initializer=\"lecun_normal\")(g)\n",
    "g = keras.layers.BatchNormalization()(g)\n",
    "g = keras.layers.Activation(activation=\"elu\")(g)\n",
    "g = keras.layers.Conv2DTranspose(filters=90, kernel_size=(3), strides=(2), padding=\"valid\", kernel_initializer=\"lecun_normal\")(g)\n",
    "g = keras.layers.BatchNormalization()(g)\n",
    "g = keras.layers.Activation(activation=\"elu\")(g)\n",
    "g = keras.layers.SeparableConv2D(filters=150, kernel_size=(2), strides=(1), padding=\"valid\", kernel_initializer=\"lecun_normal\")(g)\n",
    "g = keras.layers.BatchNormalization()(g)\n",
    "g = keras.layers.Activation(activation=\"elu\")(g)\n",
    "g = keras.layers.SeparableConv2D(filters=200, kernel_size=(2), strides=(1), padding=\"same\", kernel_initializer=\"lecun_normal\")(g)\n",
    "g = keras.layers.BatchNormalization()(g)\n",
    "g = keras.layers.Activation(activation=\"elu\")(g)\n",
    "g = keras.layers.Dense(units=80, kernel_initializer=\"lecun_normal\")(g)\n",
    "g = keras.layers.BatchNormalization()(g)\n",
    "g = keras.layers.Activation(activation=\"elu\")(g)\n",
    "g = keras.layers.Dense(units=40, kernel_initializer=\"lecun_normal\")(g)\n",
    "g = keras.layers.BatchNormalization()(g)\n",
    "g = keras.layers.Activation(activation=\"elu\")(g)\n",
    "gen_out = keras.layers.Dense(units=3, activation=\"sigmoid\")(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94da8fac-b3a7-4a58-b41f-fb685b947937",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_generator = keras.Model(inputs=[gen_in], outputs=[gen_out], name=\"gen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98032679-126f-4110-be54-247f487d8507",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f61752-7031-4230-b515-8d59e8100e8a",
   "metadata": {},
   "source": [
    "## Build the discriminator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944cad48-f2a6-464d-9af9-04a51b3d56e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_in = keras.layers.Input(shape=(218, 178, 3))\n",
    "d = keras.layers.Conv2D(filters=30, kernel_size=(3), kernel_initializer=\"lecun_normal\", padding=\"same\")(disc_in)\n",
    "d = keras.layers.Activation(activation=\"relu\")(d)\n",
    "d = keras.layers.MaxPool2D(pool_size=(2), strides=(2), padding=\"same\")(d)\n",
    "d = keras.layers.SeparableConv2D(filters=60, kernel_size=(2), kernel_initializer=\"lecun_normal\")(d)\n",
    "d = keras.layers.Activation(activation=\"relu\")(d)\n",
    "d = keras.layers.MaxPool2D(pool_size=(2), strides=(2), padding=\"same\")(d)\n",
    "d = keras.layers.SeparableConv2D(filters=90, kernel_size=(2), kernel_initializer=\"lecun_normal\")(d)\n",
    "d = keras.layers.Activation(activation=\"relu\")(d)\n",
    "d = keras.layers.MaxPool2D(pool_size=(2), strides=(2), padding=\"same\")(d)\n",
    "d = keras.layers.SeparableConv2D(filters=150, kernel_size=(2), kernel_initializer=\"lecun_normal\")(d)\n",
    "d = keras.layers.Activation(activation=\"relu\")(d)\n",
    "d = keras.layers.GlobalMaxPool2D()(d)\n",
    "disc_out = keras.layers.Dense(units=1, activation=\"sigmoid\")(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ece084a-73a9-4851-93e7-b20c68512a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_discriminator = keras.Model(inputs=[disc_in], outputs=[disc_out], name=\"disc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3d3e02-8783-48fc-af53-7e7369d4517b",
   "metadata": {},
   "source": [
    "## Customizing the `fit` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede67b77-bc0f-434f-9415-e212e91fbae8",
   "metadata": {},
   "source": [
    "In a GAN, each full training step consists of two individual training steps:\n",
    "\n",
    "- Training the discriminator\n",
    "- Training the generator\n",
    "\n",
    "The steps in training the discriminator are the following:\n",
    "- We create random, normal distributed noise\n",
    "- We pass that noise to the generator, who in turn will create the generated samples\n",
    "- We then create labels for the real and generated samples\n",
    "- We concatenate the generated samples with the real one\n",
    "- We then train the generator in a supervised fashion, i.e.\n",
    "    - passing the concatenated samples to the generator to generate his predictions,\n",
    "    - calculate the loss based on his predictions and the true labels,\n",
    "    - calculate the gradients based on this loss and apply them\n",
    "\n",
    "After that we train the generator in the following way (second part of the full training step):\n",
    "- We create random, normal distributed noise\n",
    "- We pass that noise to the generator, who in turn will create the generated samples\n",
    "- We fix the discriminator weights in place and train the generator in a supervised fashion, i.e.\n",
    "    - passing the generated samples to the GAN to generate the fake/real predictions\n",
    "    - calculate the loss based on his predictions and the true labels,\n",
    "    - calculate the gradients based on this loss and apply them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3950de-b958-406c-b8ae-52995cf8fbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGAN(keras.Model):\n",
    "    def __init__(self, generator, discriminator, noise_gen, noise_size=54600, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.noise_gen = noise_gen\n",
    "        self.noise_size = noise_size\n",
    "        \n",
    "    def compile(self, disc_optimizer, gen_optimizer, loss_fn):\n",
    "        super().compile()\n",
    "        self.disc_optimizer = disc_optimizer\n",
    "        self.gen_optimizer = gen_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        # Get the data and batch size\n",
    "        batch_size = data.shape[0]\n",
    "        \n",
    "        # Train the discriminator model\n",
    "        noise = self.noise_gen.normal(shape=[batch_size, self.noise_size])\n",
    "        X_gen = self.generator(noise)\n",
    "        y_true = tf.constant([[1.]] * batch_size + [[0.]] * batch_size)\n",
    "        y_true += 0.05 * tf.random.uniform(shape=tf.shape(y_true))\n",
    "        X_all = tf.concat([X_gen, data], axis=0)\n",
    "        with tf.GradientTape() as tape1:\n",
    "            y_pred = self.discriminator(X_all)\n",
    "            disc_loss = self.loss_fn(y_true, y_pred)\n",
    "        disc_vars = self.discriminator.trainable_variables\n",
    "        disc_gradients = tape1.gradient(disc_loss, disc_vars)\n",
    "        self.disc_optimizer.apply_gradients(zip(disc_gradients, disc_vars))\n",
    "        \n",
    "        # Train the generator model\n",
    "        noise = self.noise_gen.normal(shape=[batch_size, self.noise_size])\n",
    "        y_true = tf.constant([[0.]] * batch_size)\n",
    "        with tf.GradientTape() as tape:\n",
    "            X_gen = self.generator(noise)\n",
    "            y_pred = self.discriminator(X_gen)\n",
    "            gen_loss = self.loss_fn(y_true, y_pred)\n",
    "        gen_vars = self.generator.trainable_variables\n",
    "        gen_gradients = tape.gradient(gen_loss, gen_vars)\n",
    "        self.gen_optimizer.apply_gradients(zip(gen_gradients, gen_vars))\n",
    "        \n",
    "        return {\"disc_loss\" : disc_loss, \"gen_loss\" : gen_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79575bf2-1235-4259-a862-26a66817929c",
   "metadata": {},
   "source": [
    "## Define training parameters and compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f133da-7304-4630-9d68-2f0089e86063",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_opti = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
    "gen_opti = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
    "loss_fn = keras.losses.BinaryCrossentropy()\n",
    "noise_gen = tf.random.Generator.from_seed(seed=1224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07f4b3b-6075-413a-b725-21d96b325d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = MyGAN(generator=m_generator, discriminator=m_discriminator,\n",
    "            noise_gen=noise_gen, noise_size=noise_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59319152-1423-4eda-9c8a-390c7463a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.compile(disc_optimizer=disc_opti, gen_optimizer=gen_opti, loss_fn=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a646789f-30dc-4260-bbd3-7676713ddf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = gan.fit(x=image_dataset, epochs=20, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
